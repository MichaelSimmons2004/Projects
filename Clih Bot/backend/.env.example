# ─── LMStudio Auto-Launch ─────────────────────────────────────────────────────
# Automatically find and start LMStudio's server when CLIHBot starts.
# The user never needs to open LMStudio or pick a port — just install it once.
LMSTUDIO_AUTO_START=true

# Path to the `lms` CLI that ships with LMStudio.
# Leave blank for auto-detection (checks PATH and common install directories).
# LMSTUDIO_EXECUTABLE=C:\Users\you\AppData\Local\LM-Studio\bin\lms.exe

# Stop the LMStudio server when CLIHBot exits.
# Default: false — leave it running (other apps may be using it).
LMSTUDIO_STOP_ON_EXIT=false

# ─── LMStudio / Model ─────────────────────────────────────────────────────────
# Auto-load the model before the first message if not already in memory
MODEL_AUTO_LOAD=true

# Idle TTL in seconds — LMStudio auto-unloads the model after this many seconds
# of inactivity. The timer resets with every request. 0 = rely on LMStudio default (60 min).
MODEL_IDLE_TTL_SECONDS=600

# KV Cache quantization — reduces VRAM significantly, minimal quality loss.
# q8_0 on both K and V typically saves ~30–40% VRAM vs f16 (flash_attention is always enabled).
# Options: f32, f16, q8_0, q4_0, q4_1, iq4_nl, q5_0, q5_1 — leave blank to disable.
MODEL_K_CACHE_QUANT=q8_0
MODEL_V_CACHE_QUANT=q8_0


# Base URL of your LMStudio server (or any OpenAI-compatible endpoint)
LMSTUDIO_BASE_URL=http://localhost:1234/v1

# The model identifier as it appears in LMStudio
LMSTUDIO_MODEL=lmstudio-community/Meta-Llama-3.1-8B-Instruct-GGUF

# API key — LMStudio doesn't require one, but the client needs a non-empty value
LMSTUDIO_API_KEY=lm-studio

# Whether the loaded model supports vision (image input). true/false
MODEL_VISION_CAPABLE=false

# ─── Context Window ────────────────────────────────────────────────────────────
# Hard limit in tokens for the full context sent to the model
# Target: fits comfortably under 12 GB VRAM with Q8 KV cache on an 8B model.
# Estimated VRAM at 40k + Q8: ~10.7 GB  (55k = ~12.4 GB, too tight)
# Increase if your GPU has more VRAM and you want a longer working memory.
CONTEXT_WINDOW_TOKENS=40000

# Token budget reserved for the model's response
RESPONSE_RESERVE_TOKENS=4096

# Max tokens for any single tool result before truncation
TOOL_RESULT_MAX_TOKENS=5000

# How many terminal lines to include in the active context snapshot
TERMINAL_CONTEXT_LINES=100

# ─── API Server ───────────────────────────────────────────────────────────────
API_HOST=0.0.0.0
API_PORT=8765

# ─── Browser CDP ──────────────────────────────────────────────────────────────
# Chrome/Edge remote debugging port (launch browser with --remote-debugging-port=9222)
CDP_PORT=9222
CDP_HOST=localhost

# ─── Browser Extension ────────────────────────────────────────────────────────
# WebSocket port the extension connects to
EXTENSION_WS_PORT=8766

# ─── Terminal Capture ─────────────────────────────────────────────────────────
# Mode: "logfile" or "pty"
TERMINAL_MODE=logfile

# Log file path — OPTIONAL. Leave blank to use the smart default:
#   Run `python session.py` to start a named session. The launcher
#   writes the path to sessions/.current and the watcher picks it up
#   automatically. No manual path setting needed.
#
# Only set this if you want to force a specific fixed path regardless of sessions:
# TERMINAL_LOG_FILE=/tmp/clihbot_terminal.log

# Ring buffer size in lines (PTY mode)
TERMINAL_RING_BUFFER_LINES=2000

# ─── Security Scanner ─────────────────────────────────────────────────────────
# Minimum severity to surface in real-time alerts: critical, high, medium, low
ALERT_SEVERITY_THRESHOLD=high

# File extensions to auto-scan when read via the file tool
AUTO_SCAN_EXTENSIONS=.py,.js,.ts,.sh,.bash,.php,.rb,.go,.java,.cs,.cpp,.c
